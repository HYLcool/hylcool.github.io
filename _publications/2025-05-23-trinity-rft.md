---
title: "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models"
collection: publications
permalink: /publication/2025-05-23-trinity-rft
date: 2025-05-23
venue: 'arXiv'
citation: 'Pan, Xuchen, Yanxi Chen, Yushuo Chen, Yuchang Sun, Daoyuan Chen, Wenhao Zhang, Yuexiang Xie, <ins>Yilun Huang</ins>, Yilei Zhang, Dawei Gao, Yaliang Li, Bolin Ding, Jingren Zhou. "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models." arXiv preprint arXiv:2505.17826 (2025).'
paperurl: 'https://arxiv.org/abs/2505.17826'
code: 'https://github.com/modelscope/Trinity-RFT'
---

<strong>Abstraction</strong>: Trinity-RFT is a general-purpose, flexible and scalable framework designed for reinforcement fine-tuning (RFT) of large language models. It is built with a decoupled design, consisting of (1) an RFT-core that unifies and generalizes synchronous/asynchronous, on-policy/off-policy, and online/offline modes of RFT, (2) seamless integration for agent-environment interaction with high efficiency and robustness, and (3) systematic data pipelines optimized for RFT. Trinity-RFT can be easily adapted for diverse application scenarios, and serves as a unified platform for exploring advanced reinforcement learning paradigms. This technical report outlines the vision, features, design and implementations of Trinity-RFT, accompanied by extensive examples demonstrating the utility and user-friendliness of the proposed framework.

- Download paper [here](https://arxiv.org/abs/2505.17826)
- Code is available [here](https://github.com/modelscope/Trinity-RFT)
